{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "700940a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "global chunks\n",
    "import json\n",
    "chunks = []\n",
    "def loaddata(location = r\"C:\\Users\\VectorStorage\\Text.pdf\"):\n",
    "    from langchain.document_loaders import PyPDFLoader\n",
    "    loader = PyPDFLoader(location)\n",
    "    pages = loader.load()\n",
    "    \n",
    "    from langchain.text_splitter import CharacterTextSplitter\n",
    "    text_splitter = CharacterTextSplitter(\n",
    "        separator=\"\\n\",\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=150,\n",
    "        length_function=len\n",
    "    )\n",
    "    docs = text_splitter.split_documents(pages)\n",
    "    from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size = 1500,\n",
    "        chunk_overlap = 150\n",
    "    )\n",
    "    global chunks\n",
    "    chunks = text_splitter.split_documents(docs)\n",
    "    \n",
    "    import openai\n",
    "    openai.api_key = ''\n",
    "    embd = []\n",
    "    for chunk in chunks:\n",
    "        response = openai.Embedding.create(input= chunk.page_content, model=\"text-embedding-ada-002\")\n",
    "        embd.append(response)\n",
    "    if os.path.exists('embedding_index.faiss'):\n",
    "        os.remove('embedding_index.faiss')\n",
    "    embeddings = [emb['data'][0]['embedding'] for emb in embd]\n",
    "    embeddings = np.array(embeddings, dtype=np.float32)\n",
    "\n",
    "    d = embeddings.shape[1]\n",
    "    index = faiss.IndexFlatL2(d)\n",
    "\n",
    "    index.add(embeddings)\n",
    "\n",
    "    faiss.write_index(index, 'embedding_index.faiss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec17d9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import faiss\n",
    "import os\n",
    "import sys\n",
    "import openai\n",
    "import json\n",
    "def qna():\n",
    "    if chunks is None:\n",
    "        print(\"Error: Data is not loaded successfully. Please run 'loaddata' first.\")\n",
    "        return\n",
    "    query = input(\"Question : \")\n",
    "    response = openai.Embedding.create(input=query, model=\"text-embedding-ada-002\")\n",
    "    query_emb = response['data'][0]['embedding']\n",
    "#     query_emb = np.array(query_emb, dtype=np.float32)\n",
    "    \n",
    "    if os.path.exists('embedding_index.faiss'):\n",
    "        index = faiss.read_index('embedding_index.faiss')\n",
    "    else:\n",
    "        raise FileNotFoundError(\"Faiss index file not found.\")\n",
    "    \n",
    "    query_embedding = np.array([query_emb])\n",
    "    k = 5\n",
    "    distances, indices = index.search(query_embedding, k)\n",
    "    indices = indices[0]\n",
    "    context = ''\n",
    "    for indice in indices:\n",
    "        context = context + ' ' + chunks[indice].page_content\n",
    "        \n",
    "    with open(\"memory.json\", \"r\") as outfile:\n",
    "        json_object = json.load(outfile)\n",
    "        mem = \"\"\n",
    "        for objects in json_object:\n",
    "            mem = mem + \"Quesion - \" + objects[\"question\"] + \"Answer - \" + objects[\"answer\"]\n",
    "            \n",
    "    \n",
    "    messages = [\n",
    "        {\n",
    "        'role': 'system',\n",
    "        'content': f\"Use only the following pieces of context to answer the question at the end. \"\n",
    "                    f\"Do not make answers by yourself, just give the answers with respect to the context. \"\n",
    "                    f\"If the question is not with respect to the context, just give 'I don't know' as output and don't make answers. \\n \"\n",
    "                    f\"If you don't know the answer, just say that you don't know. Use three sentences at maximum. \"\n",
    "                    f\"Keep the answer as concise as possible. Always say 'thanks for asking!' at the end of the answer. \"\n",
    "                    f\"Additionally, consider the previous 4 conversations for a follow-up question:\\n\\n {mem} \\n\"\n",
    "                    f\"Do not make answers by yourself, just give the answers with respect to the context. \"\n",
    "                    f\"If the question is not with respect to the context, just give 'I don't know' as output and don't make answers. \\n \"\n",
    "                    f\"\\n  Context: {context}\"\n",
    "        },\n",
    "        {'role': 'user', 'content': f\"{query}\"}\n",
    "    ]\n",
    "\n",
    "\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=messages,\n",
    "        temperature=0,\n",
    "    )\n",
    "    answer = response['choices'][0]['message']['content']\n",
    "    \n",
    "    print(\"Answer->\",response['choices'][0]['message']['content'])\n",
    "    try:\n",
    "        with open(\"memory.json\", \"r\") as infile:\n",
    "            memory = json.load(infile)\n",
    "    except (FileNotFoundError, json.decoder.JSONDecodeError):\n",
    "        memory = []\n",
    "    \n",
    "    cache = {\n",
    "        \"question\": query,\n",
    "        \"answer\": answer\n",
    "    }\n",
    "    memory.append(cache)\n",
    "    \n",
    "    memory = memory[-4:]\n",
    "    \n",
    "    with open(\"memory.json\", \"w\") as outfile:\n",
    "        json.dump(memory, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb3a34e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaddata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0289dc85",
   "metadata": {},
   "outputs": [],
   "source": [
    "qna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea62025",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4439cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4246ba67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c30f44f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92cee0ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11528443",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bcfc626",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c4acb1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b79d44ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0efd17e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b52143c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0341fbc3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1878ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18601e33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d59b266",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b59e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "#     messages2 = [\n",
    "#         {'role': 'system', 'content': f\"Use the text to figure out if the question is related to a previous question or not. \\\n",
    "#          Just check if the question given by the user is relevant to the previous questions or not, and then after this give \\\n",
    "#          the response as a full regenerated question which is self sufficient to understand and can be the full question. \\\n",
    "#          directly. If the new question is not related to the previous questions then just pass the \\\n",
    "#          question as it is. \\n {mem}\"},\n",
    "#         {'role': 'user', 'content': f\"{query}\"}\n",
    "#     ]\n",
    "    \n",
    "#     response = openai.ChatCompletion.create(\n",
    "#         model=\"gpt-3.5-turbo\",\n",
    "#         messages=messages2,\n",
    "#         temperature=0,\n",
    "#     )\n",
    "#     queryf = response['choices'][0]['message']['content']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
